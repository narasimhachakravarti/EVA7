# Session 14.0 DETR End to End Object detection with transformers

## DETR fine tuned with balloon dataset

## Code Links

Colab Link : https://colab.research.google.com/drive/1GHr8QiJYIr_-h74pZgcDkhVVkDjZbyl4#scrollTo=L5DlkJmsUk3n

Github Link : https://github.com/narasimhachakravarti/EVA7/blob/main/Session%2014/Readme.md

## Explanation

1. DETR: With DETR the objective is to detect objects in the image and their bounding boxes. DETR permits max 100 objects to be detected and always emits 100 outputs. This is done in parallel in single pass. Some of the outputs may be (no_object, bb) type if there are less than 100 objects in the image. DETR uses transformer to encode and decode. Since it must emit two variables - class and bounding box, the decoder is followed by a feed forward network (one network for each of the 100 queries) which emits two variables class and bb.

2. Encoder-Decoder Architecture: We have previously seen that the encoder in case of VIT takes patches of the input image, flattens across RGB and uses them as input embeddings. In case of DETR, the encoder instead takes the feature vector generated by a resnet encoder. The activation output of final layer of resnet encoder downscales the image by a factor of 32, and adds 2048 channels. So if we use an image of size 640x640, we would get an embedding of size 20x20x2048 which is unravelled (H x W) to 400x2048 and then reduced by a linear layer to 400x256. To this input, an equal sized vector randomly initialized is added as the position encoding. There is no CTS input. The encoder does self attention on the inputs via QKV concept, which is then passed on to a FFN layer (self attention does not perform softmax) to provide another opportunity for inputs to wiggle. Both encoder and decoer have multiple attention heads, each of which may focus on different aspects and relationship in the image. This happens in parallel over the multiple attention heads.

3. Decoder and Object queries: We expect output of the decoder to answer questions of objects and their bb. The decoder is hinted to provide 100 outputs via 100 randomly initialized object queries. Each query will then become the vehicle via which one of the outputs will be conveyed. The initial set of 100 queries is subject to a self attention layer to refine them. The next step is to blend the queries with encoder output which is mapped to value and key for the second attention block in the decoder. Encoder output is directly mapped as V, and blended with position encoding to be used as K. All this goes through the second multi head attention block. The outputs of this block number 100 (same as input)

4. Bipartite loss: To compute loss function, we need to match outputs to ground truth. Since there is no inherent ordering for objects in image, the ground truth and output do not have a direct 1-1 correspondence. So each decoder output (class, bb) is scanned against the closest (class, bb) in the ground truth. Then then next decoder outputs is matched against the remaining ground truths and so on. This bipartite matching is called Hungarian algorithm. Note that it is possible for output to match a bb, but not its class. In this case the loss function will then drive the class to be corrected.

## Result

100%
313/313 [02:38<00:00, 2.26it/s]
Epoch : 1 - loss : 0.6713 - acc: 0.5798 - val_loss : 0.6722 - val_acc: 0.5831

100%
313/313 [02:38<00:00, 2.24it/s]
Epoch : 2 - loss : 0.6600 - acc: 0.5926 - val_loss : 0.6506 - val_acc: 0.6119

100%
313/313 [02:38<00:00, 2.30it/s]
Epoch : 3 - loss : 0.6502 - acc: 0.6085 - val_loss : 0.6517 - val_acc: 0.6019

100%
313/313 [02:38<00:00, 2.28it/s]
Epoch : 4 - loss : 0.6463 - acc: 0.6115 - val_loss : 0.6438 - val_acc: 0.6185

100%
313/313 [02:38<00:00, 2.29it/s]
Epoch : 5 - loss : 0.6420 - acc: 0.6218 - val_loss : 0.6396 - val_acc: 0.6268

100%
313/313 [02:38<00:00, 2.28it/s]
Epoch : 6 - loss : 0.6339 - acc: 0.6305 - val_loss : 0.6262 - val_acc: 0.6383

100%
313/313 [02:38<00:00, 2.32it/s]
Epoch : 7 - loss : 0.6272 - acc: 0.6406 - val_loss : 0.6266 - val_acc: 0.6331

100%
313/313 [02:38<00:00, 2.34it/s]
Epoch : 8 - loss : 0.6231 - acc: 0.6429 - val_loss : 0.6145 - val_acc: 0.6472

100%
313/313 [02:38<00:00, 2.25it/s]
Epoch : 9 - loss : 0.6190 - acc: 0.6461 - val_loss : 0.6371 - val_acc: 0.6145

100%
313/313 [02:38<00:00, 2.27it/s]
Epoch : 10 - loss : 0.6104 - acc: 0.6553 - val_loss : 0.6145 - val_acc: 0.6570

100%
313/313 [02:39<00:00, 2.26it/s]
Epoch : 11 - loss : 0.6071 - acc: 0.6608 - val_loss : 0.6120 - val_acc: 0.6564

100%
313/313 [02:37<00:00, 2.30it/s]
Epoch : 12 - loss : 0.6021 - acc: 0.6663 - val_loss : 0.6007 - val_acc: 0.6744

100%
313/313 [02:36<00:00, 2.31it/s]
Epoch : 13 - loss : 0.5974 - acc: 0.6702 - val_loss : 0.6110 - val_acc: 0.6616

100%
313/313 [02:36<00:00, 2.32it/s]
Epoch : 14 - loss : 0.5966 - acc: 0.6740 - val_loss : 0.5943 - val_acc: 0.6703

100%
313/313 [02:37<00:00, 2.27it/s]
Epoch : 15 - loss : 0.5945 - acc: 0.6769 - val_loss : 0.5906 - val_acc: 0.6770

100%
313/313 [02:37<00:00, 2.28it/s]
Epoch : 16 - loss : 0.5883 - acc: 0.6810 - val_loss : 0.5979 - val_acc: 0.6764

100%
313/313 [02:37<00:00, 2.35it/s]
Epoch : 17 - loss : 0.5870 - acc: 0.6848 - val_loss : 0.5883 - val_acc: 0.6816

100%
313/313 [02:37<00:00, 2.30it/s]
Epoch : 18 - loss : 0.5837 - acc: 0.6816 - val_loss : 0.5862 - val_acc: 0.6839

100%
313/313 [02:37<00:00, 2.30it/s]
Epoch : 19 - loss : 0.5803 - acc: 0.6893 - val_loss : 0.5917 - val_acc: 0.6828

100%
313/313 [02:37<00:00, 2.26it/s]
Epoch : 20 - loss : 0.5828 - acc: 0.6842 - val_loss : 0.5867 - val_acc: 0.6784
